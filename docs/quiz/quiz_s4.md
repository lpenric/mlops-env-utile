# Quiz S4 ‚Äî MLflow Tracking

**Dur√©e estim√©e** : 15 minutes  
**Seuil de r√©ussite** : 7/10

## Partie A ‚Äî Faire (6 questions QCM)

### Question 1
Tu viens de lancer `make train SERVICE=uhi_service`. Dans quel dossier MLflow stocke-t-il les informations du run (params, metrics, artefacts) ?

- A) `services/uhi_service/models/`
- B) `./mlruns/0/[run_id]/`
- C) `/tmp/mlflow/`
- D) `~/.mlflow/experiments/`

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B**

**Explication** : MLflow utilise `./mlruns/` √† la racine du projet par d√©faut. L'experiment ID 0 (Default) cr√©e un sous-dossier `0/`, et chaque run a son propre sous-dossier identifi√© par un UUID (ex: `da7ff42a...`).

**Anti-pattern** : Chercher les runs dans `services/` ou confondre avec le mod√®le .pkl sauvegard√© manuellement (qui va dans `services/uhi_service/models/`).
</details>

---

### Question 2
L'interface MLflow UI sur `http://localhost:5000` affiche une page blanche. Quelle est la cause la plus probable sur Mac ?

- A) MLflow n'est pas install√©
- B) Le port 5000 est occup√© par un autre process
- C) Probl√®me de r√©solution DNS avec `localhost`
- D) Firewall bloque le port

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : C**

**Explication** : Sur Mac, `localhost` peut avoir des probl√®mes de r√©solution DNS. L'API MLflow r√©pond correctement (testable avec `curl`), mais le frontend ne charge pas. Solution : utiliser `http://127.0.0.1:5000` (IP loopback directe).

**Erreur fr√©quente** : Relancer MLflow ou changer de port alors que le serveur fonctionne. Toujours tester l'API avec `curl` avant.

**Commande diagnostic** : `curl "http://127.0.0.1:5000/api/2.0/mlflow/experiments/get?experiment_id=0"` (guillemets obligatoires √† cause du `?` en zsh).
</details>

---

### Question 3
Tu veux r√©initialiser compl√®tement MLflow pour repartir de z√©ro. Quelle est la m√©thode correcte ?

- A) `rm -rf mlruns/*`
- B) `rm -rf mlruns` puis `make train`
- C) `mlflow experiments delete --experiment-id 0`
- D) `rm -rf mlruns` puis `mkdir mlruns`

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B**

**Explication** : `rm -rf mlruns` supprime tout. Au prochain `make train`, MLflow recr√©e automatiquement la structure (`mlruns/0/meta.yaml`, etc.). C'est la m√©thode la plus propre.

**Anti-pattern A** : `rm -rf mlruns/*` laisse un dossier `mlruns/` vide. MLflow refuse de r√©initialiser (s√©curit√©) et g√©n√®re l'erreur "Could not find experiment with ID 0".

**Anti-pattern D** : Cr√©er `mlruns/` manuellement avant le premier run bloque aussi la r√©initialisation auto.

**R√®gle** : Si `mlruns/` n'existe pas ‚Üí MLflow le cr√©e. Si `mlruns/` existe ‚Üí MLflow suppose qu'il contient des donn√©es importantes.
</details>

---

### Question 4
Dans ton code `train.py`, tu as ajout√© `mlflow.log_param("max_depth", 10)`. O√π retrouves-tu cette valeur dans MLflow UI ?

- A) Onglet "Metrics"
- B) Onglet "Parameters" (ou colonne `max_depth` dans la liste des runs)
- C) Onglet "Artifacts"
- D) Onglet "Tags"

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B**

**Explication** : `log_param()` enregistre un hyperparam√®tre (valeur fix√©e avant l'entra√Ænement). MLflow l'affiche dans la section "Parameters" de chaque run, et permet de cr√©er une colonne `max_depth` dans le tableau de comparaison.

**Diff√©rence cl√©** :
- `log_param()` ‚Üí hyperparams (max_depth, n_estimators)
- `log_metric()` ‚Üí m√©triques de performance (mae, r2_score)
- `log_model()` ‚Üí artefact mod√®le (fichier pkl)

**Astuce UI** : Cliquer sur "Columns" pour afficher/masquer les colonnes params et metrics.
</details>

---

### Question 5
Tu obtiens l'erreur `MlflowException: Could not find experiment with ID 0` au lancement de `make train`. Quelle est la solution imm√©diate ?

- A) R√©installer MLflow : `conda install mlflow`
- B) Cr√©er l'experiment : `mlflow experiments create --experiment-name "Default"`
- C) Supprimer et recr√©er : `rm -rf mlruns` puis `make train`
- D) Changer l'experiment ID dans le code

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : C**

**Explication** : Cette erreur survient quand `mlruns/` existe mais que `mlruns/0/meta.yaml` est absent (structure corrompue). La solution la plus simple : supprimer compl√®tement `mlruns/` et laisser MLflow le recr√©er proprement.

**Option B fonctionne aussi** mais n√©cessite la bonne syntaxe : `mlflow experiments create --experiment-name "Default" --artifact-location file://$(pwd)/mlruns/0`. Option C est plus rapide.

**Erreur fr√©quente** : Avoir fait `rm -rf mlruns/*` puis `mkdir mlruns` avant l'erreur. C'est justement ce qui casse la structure.
</details>

---

### Question 6
Tu compares 2 runs dans MLflow UI. Run 1 a `max_depth=10, mae=4.74`. Run 2 a `max_depth=15, mae=4.75`. Que peux-tu en conclure ?

- A) Le mod√®le avec max_depth=15 est meilleur (plus profond = mieux)
- B) Le mod√®le avec max_depth=10 est meilleur (MAE plus faible)
- C) Les 2 mod√®les sont √©quivalents (diff√©rence n√©gligeable)
- D) Impossible de conclure sans voir R¬≤ et les donn√©es de test

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : C (ou D acceptable)**

**Explication** : Diff√©rence de MAE de 0.01 sur un score ~4.7 est **n√©gligeable** (¬±0.2%). Sur des donn√©es synth√©tiques avec bruit al√©atoire, c'est du bruit statistique. Les 2 mod√®les performent pareil.

**R√©ponse D acceptable** car en production, on regarde aussi :
- R¬≤ (variance expliqu√©e)
- Performance sur donn√©es de test (pas juste train)
- Latence d'inf√©rence (max_depth=10 peut √™tre plus rapide)
- Co√ªt de r√©entra√Ænement

**Anti-pattern B** : Choisir syst√©matiquement le MAE le plus bas sans contexte. En ML, une diff√©rence <1% est souvent du bruit.

**Lien S5** : Le Model Registry (semaine prochaine) aide √† comparer et choisir la version √† mettre en production.
</details>

---

## Partie B ‚Äî Comprendre (4 questions ouvertes)

### Question 7
Explique en 3-4 phrases : pourquoi MLflow Tracking est utile dans un projet ML ? Que se passerait-il sans ?

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse attendue** (3 points cl√©s) :

1. **Tra√ßabilit√©** : MLflow enregistre automatiquement tous les runs (params, m√©triques, artefacts). Sans lui, tu devrais noter manuellement "quel mod√®le j'ai entra√Æn√© avec quels hyperparams", ce qui est source d'erreurs.

2. **Comparaison** : L'interface permet de comparer visuellement plusieurs runs (ex: max_depth=10 vs 15). Sans outil, tu dois relire des logs texte √©parpill√©s ou refaire les entra√Ænements.

3. **Reproductibilit√©** : Chaque run sauvegarde le mod√®le + metadata. Si un mod√®le performe bien en prod, tu peux retrouver exactement comment il a √©t√© entra√Æn√©. Sans MLflow, risque de "on sait plus quel mod√®le on a d√©ploy√©".

**Lien avec valeur client** : En cas de r√©gression en production (mod√®le v2 moins bon que v1), MLflow permet un rollback imm√©diat vers v1. Sans tracking, c'est la panique ("c'√©tait quoi les bons hyperparams d√©j√† ?").

**Anticipation S5** : Le Model Registry (semaine prochaine) ajoute une couche au-dessus : versions nomm√©es (v1, v2) avec √©tats (Staging, Production) pour g√©rer le cycle de vie des mod√®les.
</details>

---

### Question 8
Tu as lanc√© `make train` 4 fois aujourd'hui. MLflow affiche 4 runs dans l'UI. Un client te demande "pourquoi garder tous ces runs, √ßa prend de la place ?". Comment r√©ponds-tu ?

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse attendue** (3 points cl√©s) :

1. **Historique d'exp√©rimentation** : Les 4 runs montrent ton processus d'optimisation (ex: tester diff√©rents hyperparams). C'est la preuve que tu as it√©r√© et compar√© scientifiquement, pas choisi au hasard.

2. **Audit & conformit√©** : En environnement pro (banque, sant√©, √©nergie), tu dois pouvoir justifier "pourquoi ce mod√®le en production ?". MLflow fournit la trace compl√®te : qui, quand, avec quels params, quelle performance.

3. **Co√ªt n√©gligeable** : Un run MLflow = quelques Ko de metadata + 1 fichier .pkl (~1-5 Mo). 100 runs = ~500 Mo max. C'est bien moins cher que de refaire un entra√Ænement perdu.

**Pitch client** : "C'est comme un journal de bord automatique. Si votre mod√®le r√©gresse en production, je peux retrouver instantan√©ment le bon mod√®le et ses param√®tres, au lieu de passer 2 jours √† r√©exp√©rimenter. √áa r√©duit votre risque op√©rationnel de 80%."

**Note** : En pratique, on peut nettoyer les runs de test (dur√©e <1s, pas de metrics) apr√®s validation. Mais garder tous les runs complets est la bonne pratique.
</details>

---

### Question 9
D√©cris en 4 √©tapes le flux complet entre `make train` et voir le run dans MLflow UI. (Focus : que se passe-t-il sous le capot ?)

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse attendue** (flux logique) :

1. **Lancement** : `make train` ex√©cute `python services/uhi_service/src/train.py`. Python d√©marre, imports charg√©s (numpy, sklearn, mlflow).

2. **Enregistrement** : `with mlflow.start_run():` cr√©e un nouveau run dans `./mlruns/0/[run_id]/`. MLflow g√©n√®re un UUID unique, cr√©e la structure de dossiers.

3. **Logging** : Pendant l'entra√Ænement, `mlflow.log_param()` √©crit dans `params/`, `mlflow.log_metric()` dans `metrics/`, `mlflow.sklearn.log_model()` sauvegarde le mod√®le dans `artifacts/model/`.

4. **UI** : `mlflow ui` lit le contenu de `./mlruns/0/` (tous les sous-dossiers run_id), parse les fichiers metadata, et affiche le tableau dans le navigateur. Chaque rechargement de page relit `mlruns/` (pas de cache base de donn√©es pour l'instant).

**Validation** : Mention de `./mlruns/[run_id]/`, distinction params/metrics/artifacts, et compr√©hension que l'UI lit directement le syst√®me de fichiers (pas de serveur s√©par√© en mode local).

**Lien avec S5** : Le Model Registry ajoute une couche "registered models" avec versions nomm√©es, mais utilise toujours `mlruns/` en backend.
</details>

---

### Question 10
Un coll√®gue d√©butant te dit : "J'ai fait `rm -rf mlruns/*`, maintenant MLflow plante. Mais hier √ßa marchait !" Explique-lui en langage simple pourquoi et comment r√©parer.

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse attendue** (explication p√©dagogique) :

1. **Pourquoi √ßa casse** : `rm -rf mlruns/*` supprime le contenu (les runs + le fichier `meta.yaml` de l'experiment 0) mais laisse le dossier `mlruns/` vide. MLflow voit que `mlruns/` existe et suppose qu'il contient une structure valide, mais ne trouve pas l'experiment 0 ‚Üí erreur.

2. **Analogie** : C'est comme vider compl√®tement un classeur mais laisser le meuble vide. Quand tu cherches le dossier "Default", tu trouves le meuble mais pas le dossier ‚Üí confusion.

3. **Solution** : Supprime le dossier entier (`rm -rf mlruns`), pas juste le contenu. Au prochain `make train`, MLflow dira "ah, pas de classeur, je dois en cr√©er un neuf" et reconstruit tout proprement.

**R√®gle simple** : Pour reset MLflow, toujours faire `rm -rf mlruns` (supprimer le dossier), jamais `rm -rf mlruns/*` (vider le contenu).

**Validation** : Si ton coll√®gue peut expliquer √† quelqu'un d'autre avec ses propres mots (sans r√©citer ta r√©ponse), c'est gagn√©. Sinon, lui faire tester la manipulation pour comprendre visc√©ralement.
</details>

---

## Bar√®me & Interpr√©tation

| Score | Niveau | Action |
|-------|--------|--------|
| 9-10 | ‚≠ê‚≠ê‚≠ê Excellent | Pr√™t pour S5 (Model Registry) |
| 7-8 | ‚≠ê‚≠ê Bien | R√©viser questions rat√©es, puis S5 |
| 5-6 | ‚≠ê Fragile | Relancer 2 runs MLflow + explorer UI avant S5 |
| 0-4 | ‚ö†Ô∏è Lacunes | Refaire TP S4 (train + UI + reset), puis refaire quiz |

## Auto-√©valuation

**Questions √† se poser apr√®s le quiz** :
- Peux-tu expliquer oralement (sans notes) le flux `make train` ‚Üí MLflow UI √† un coll√®gue ?
- Si MLflow plante demain, peux-tu diagnostiquer en <5 min (logs, curl, structure dossiers) ?
- Peux-tu pitcher la valeur de MLflow Tracking √† un client en 2 phrases (focus b√©n√©fice m√©tier, pas technique) ?

Si 3 √ó oui ‚Üí **S4 solidement acquise** üéâ  
Si 2/3 ‚Üí Revoir les concepts flous (questions ouvertes rat√©es)  
Si ‚â§1 ‚Üí Refaire le TP S4 avant de passer √† S5