# Quiz S3 ‚Äî Tests fumig√®ne (pytest) & Validation Pydantic

**Dur√©e estim√©e** : 15 minutes  
**Seuil de r√©ussite** : 7/10

---

## Partie A ‚Äî Faire (6 questions QCM)

### Question 1
Tu lances `make serve SERVICE=uhi_service` mais l'API refuse de d√©marrer avec l'erreur `[Errno 48] Address already in use`. Quelle est la meilleure solution ?

- A) Red√©marrer le Mac
- B) `lsof -ti:8000 | xargs kill -9`
- C) Changer le code de l'API pour √©couter sur port 8001
- D) `make serve PORT=8000` (relancer la m√™me commande)

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B**

**Explication** : Le port 8000 est d√©j√† occup√© par un processus Uvicorn pr√©c√©dent (souvent lanc√© en arri√®re-plan avec `&`). `lsof -ti:8000` trouve le PID du processus, `xargs kill -9` le tue.

**Pourquoi pas les autres ?**
- A) Red√©marrer le Mac est overkill et prend 2 min au lieu de 2 secondes
- C) Modifier le code n'est pas n√©cessaire, le Makefile supporte `PORT=`
- D) Relancer la m√™me commande reproduira l'erreur

**Alternative valide** : `make serve PORT=8001` (utilise un port diff√©rent sans tuer l'ancien processus)

**Erreur fr√©quente** : Taper `lsof -ti/8000` (slash au lieu de deux-points) ‚Üí erreur "unknown protocol name"
</details>

---

### Question 2
Tu veux tester que ton API rejette correctement un input invalide (ndvi=99.9 au lieu de [-1, 1]). Quel code HTTP doit retourner l'endpoint `/predict` ?

- A) 200 OK
- B) 400 Bad Request
- C) 422 Unprocessable Entity
- D) 500 Internal Server Error

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : C**

**Explication** : FastAPI + Pydantic retournent automatiquement **422 Unprocessable Entity** quand la validation √©choue (contraintes `Field(ge=, le=)` non respect√©es). C'est la convention REST pour "syntaxe JSON valide, mais donn√©es invalides".

**Pourquoi pas les autres ?**
- A) 200 = succ√®s ‚Üí signifierait que l'API accepte n'importe quelle valeur (bug)
- B) 400 = requ√™te malform√©e (ex: JSON cass√©) ‚Üí ici le JSON est valide, c'est la valeur qui pose probl√®me
- D) 500 = erreur serveur (bug dans le code) ‚Üí ici c'est une erreur client (input invalide)

**Validation dans le test** :
```python
assert response.status_code == 422
assert "detail" in response.json()  # Message d'erreur de validation
```
</details>

---

### Question 3
Tu lances `pytest -q` mais vois le message `ERROR: file or directory not found: tests/test_smoke.py`. Quelle est la cause la plus probable ?

- A) pytest n'est pas install√© dans l'environnement conda
- B) Le fichier `test_smoke.py` a une faute de frappe dans le nom
- C) Tu es dans le mauvais dossier (pas √† la racine du projet)
- D) Le fichier `tests/__init__.py` est manquant

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : C**

**Explication** : pytest cherche le fichier depuis le **dossier courant**. Si tu es dans `services/uhi_service/app/`, pytest cherche `services/uhi_service/app/tests/test_smoke.py` qui n'existe pas. Solution : `cd ~/mlops_env_utile` avant de lancer pytest.

**Pourquoi pas les autres ?**
- A) Si pytest n'√©tait pas install√©, l'erreur serait `command not found: pytest`
- B) Une faute de frappe donnerait aussi "file not found", mais c'est moins fr√©quent que le mauvais dossier
- D) `__init__.py` n'est pas obligatoire pour pytest (utile pour imports, mais pas bloquant)

**V√©rification rapide** : `pwd` doit afficher `/Users/enriclapa/mlops_env_utile`

**Anti-pattern** : Se d√©placer dans les sous-dossiers pour consulter des fichiers, puis oublier de revenir √† la racine pour les commandes `make` et `pytest`.
</details>

---

### Question 4
Apr√®s avoir modifi√© `services/uhi_service/app/main.py`, tu fais `git add /services/uhi_service/app/main.py` mais Git retourne `fatal: Invalid path '/services'`. Pourquoi ?

- A) Le fichier n'existe pas
- B) Git ne supporte pas les chemins avec plusieurs niveaux de sous-dossiers
- C) Le `/` initial indique un chemin absolu (racine du disque) au lieu de relatif
- D) Il faut faire `git add .` pour ajouter tous les fichiers modifi√©s

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : C**

**Explication** : En Git, `/services/` = chemin **absolu** depuis la racine du disque (√©quivalent de `C:\services` sur Windows). Git cherche `/services/` √† la racine du Mac, qui n'existe pas. La syntaxe correcte est `git add services/uhi_service/app/main.py` (sans `/` initial) = chemin **relatif** depuis le dossier courant.

**Pourquoi pas les autres ?**
- A) Si le fichier n'existait pas, l'erreur serait diff√©rente (pathspec did not match any files)
- B) Git supporte sans probl√®me les arborescences complexes
- D) `git add .` fonctionne, mais ajoute TOUS les fichiers modifi√©s (pas toujours souhait√©)

**R√®gle √† retenir** : En Git, toujours utiliser des chemins relatifs (sans `/` au d√©but).

**Syntaxes valides** :
- `git add services/uhi_service/app/main.py` (relatif)
- `git add ./services/uhi_service/app/main.py` (relatif explicite)
</details>

---

### Question 5
Tu as fait un commit S3, puis un `git reset --hard HEAD~1` par erreur, perdant ton travail. Comment r√©cup√©rer le commit "perdu" ?

- A) `git pull origin main` (t√©l√©charger depuis GitHub)
- B) `git reflog -10` puis `git reset --hard <hash_commit_s3>`
- C) Impossible, le commit est d√©finitivement perdu
- D) `git revert HEAD` (annuler l'annulation)

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : B**

**Explication** : `git reflog` garde l'historique complet de tous les mouvements de HEAD, m√™me apr√®s reset/rebase. Le commit S3 est "d√©tach√©" mais existe encore en m√©moire (~30 jours). `git reflog -10` affiche les 10 derniers mouvements, tu identifies le hash du commit S3, puis `git reset --hard <hash>` revient √† cet √©tat.

**Pourquoi pas les autres ?**
- A) `git pull` fonctionne **seulement si tu avais push√© avant** le reset. Si le commit est uniquement local, pull ne ram√®ne rien.
- C) Git garde tout en m√©moire pendant ~30 jours, rien n'est d√©finitivement perdu (sauf si git gc force)
- D) `git revert` cr√©e un nouveau commit qui annule les changements, mais ne r√©cup√®re pas un commit perdu

**Exemple reflog** :
```
4dbbc80 HEAD@{1}: commit: S3: Tests pytest
45c0771 HEAD@{2}: reset: moving to HEAD~1
```
‚Üí Le commit 4dbbc80 est r√©cup√©rable avec `git reset --hard 4dbbc80`

**Commande bonus** : `git cherry-pick <hash>` (appliquer ce commit sur la branche actuelle sans reset complet)
</details>

---

### Question 6
Tu veux ajouter le dossier `tests/` √† un commit existant (d√©j√† push√© sur GitHub) sans cr√©er un nouveau commit. Quelle s√©quence de commandes ?

- A) `git add tests/ && git commit -m "Ajout tests"`
- B) `git add tests/ && git commit --amend --no-edit && git push`
- C) `git add tests/ && git commit --amend --no-edit && git push --force`
- D) `git add tests/ && git push`

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse : C**

**Explication** : `git commit --amend` modifie le dernier commit au lieu d'en cr√©er un nouveau. `--no-edit` garde le message de commit existant. **Comme l'historique est modifi√©**, le push normal sera rejet√© (conflict). Il faut `git push --force` pour √©craser la version distante.

**Pourquoi pas les autres ?**
- A) Cr√©e un **nouveau** commit au lieu de modifier l'existant
- B) `git push` (sans --force) sera rejet√© par GitHub avec `! [rejected] main -> main (non-fast-forward)`
- D) Il manque l'√©tape `commit --amend`, le push √©chouera

**Avertissement** : `git push --force` est **destructif** si quelqu'un d'autre a d√©j√† pull le commit. En solo (ou en coordination), c'est OK.

**Alternative sans force push** : Cr√©er un nouveau commit `"S3: Ajout tests/"` (option A), plus s√ªr mais historique moins propre.
</details>

---

## Partie B ‚Äî Comprendre (4 questions ouvertes)

### Question 7
Explique en langage naturel (sans code) ce qui se passe entre le moment o√π tu lances `pytest -q tests/test_smoke.py` et l'affichage de `3 passed in 0.11s`.

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse attendue** (3 points cl√©s) :
1. **D√©couverte** : pytest analyse le fichier `test_smoke.py`, trouve toutes les fonctions commen√ßant par `test_*` (3 fonctions ici)
2. **Setup + Ex√©cution** : Pour chaque test, pytest injecte les fixtures (ex: `api_url`), lance la fonction, v√©rifie que les `assert` passent (pas d'exception lev√©e)
3. **Rapport** : Si tous les asserts sont OK, pytest affiche `3 passed`. Si un assert √©choue, pytest affiche le d√©tail de l'erreur (ligne, valeur attendue vs re√ßue)

**Validation** :
- Mention du d√©couverte automatique (pattern `test_*`)
- Mention des fixtures (injection automatique)
- Mention des assertions (validation des r√©sultats)

**Lien avec valeur client** : "Les tests s'ex√©cutent en quelques secondes et d√©tectent automatiquement les r√©gressions, sans intervention manuelle. √áa divise par 10 le temps de validation avant mise en prod."
</details>

---

### Question 8
Pourquoi Pydantic avec `Field(ge=-1.0, le=1.0)` est mieux que faire `if ndvi < -1 or ndvi > 1: raise Exception` dans le code de `/predict` ?

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse attendue** (3 points cl√©s) :
1. **D√©claratif vs imp√©ratif** : Avec Pydantic, tu **d√©clares** les contraintes une fois dans le sch√©ma. Avec des `if`, tu dois r√©p√©ter la validation partout (duplication, risque d'oubli).
2. **Automatisation** : FastAPI valide automatiquement **avant** que le code de `/predict` s'ex√©cute. Si invalide, FastAPI retourne 422 sans toucher ton code ‚Üí s√©paration des responsabilit√©s.
3. **Documentation auto** : Les contraintes Pydantic apparaissent dans `/docs` (Swagger UI), le client voit imm√©diatement les limites accept√©es. Avec des `if`, rien n'est document√©.

**Bonus** : Messages d'erreur standardis√©s (format JSON coh√©rent avec `detail`), pas besoin de g√©rer les exceptions manuellement.

**Lien avec S4+** : En S4, MLflow va logger les contraintes Pydantic automatiquement, tra√ßabilit√© compl√®te des entr√©es accept√©es.
</details>

---

### Question 9
Un d√©veloppeur junior te dit : "Je fais toujours `git add .` puis `git commit -m "WIP"` puis `git push`, m√™me si le code ne marche pas. C'est OK ?". Que lui r√©ponds-tu ? (Avantages/inconv√©nients)

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse attendue** (3 points cl√©s) :
1. **Avantages** : Commit fr√©quent = backup cloud automatique (protection contre perte de donn√©es), historique d√©taill√© du parcours (utile pour reflog/git bisect), permet de rollback rapidement si fausse route.
2. **Inconv√©nients** : Historique Git "sale" (plein de commits WIP), difficile de comprendre l'√©volution du projet, complique les code reviews (beaucoup de bruit).
3. **Recommandation √©quilibr√©e** : Commit WIP local OK pour prot√©ger le travail, mais **avant de push** : soit squash (fusionner commits WIP), soit rebase interactif pour nettoyer l'historique. Ou utiliser des branches feature (WIP sur branche, merge propre sur main).

**Validation** : Mention du trade-off backup vs propret√©, pas de r√©ponse dogmatique ("TOUJOURS" ou "JAMAIS").

**Contexte MLOps** : En production, on veut un historique propre (1 commit = 1 feature/fix test√©e). En d√©veloppement solo, WIP fr√©quent est acceptable tant qu'on nettoie avant livraison client.
</details>

---

### Question 10
Un client te demande : "Qu'as-tu livr√© cette semaine S3 et pourquoi c'est utile pour mon projet d'√Ælots de chaleur urbains ?". R√©ponds en 2-3 phrases, focus valeur m√©tier (pas de jargon technique).

<details>
<summary>Voir la r√©ponse</summary>

**R√©ponse attendue** (√©l√©ments cl√©s) :
1. **Livrable** : "J'ai mis en place des tests automatiques qui v√©rifient que l'API refuse les donn√©es aberrantes et retourne toujours des pr√©dictions coh√©rentes."
2. **B√©n√©fice** : "√áa √©limine 90% des bugs avant la mise en production et garantit que vos agents terrain re√ßoivent des alertes fiables, pas des scores UHI n√©gatifs ou sup√©rieurs √† 100."
3. **Impact temps/co√ªt** : "Plus besoin de tester manuellement √† chaque modification, √ßa divise par 5 le temps de validation et r√©duit le risque de pannes."

**Validation** :
- Pas de mention "pytest", "Pydantic", "422" ‚Üí langage client
- Focus sur **cons√©quence concr√®te** (fiabilit√©, gain de temps, r√©duction risque)
- Lien avec le domaine m√©tier (√Ælots de chaleur, agents terrain)

**Anti-pattern** : "J'ai install√© pytest et cr√©√© 3 tests unitaires avec des fixtures et des assertions sur les codes HTTP" ‚Üí trop technique, pas de valeur m√©tier visible.
</details>

---

## Bar√®me & Interpr√©tation

| Score | Niveau | Action |
|-------|--------|--------|
| 9-10 | ‚≠ê‚≠ê‚≠ê Excellent | Pr√™t pour S4 (MLflow Tracking) |
| 7-8 | ‚≠ê‚≠ê Bien | R√©viser questions rat√©es (relire r√©ponses d√©taill√©es), puis S4 |
| 5-6 | ‚≠ê Fragile | Refaire le TP S3 (relancer tests, Git amend), comprendre le flux avant S4 |
| 0-4 | ‚ö†Ô∏è Lacunes | Revoir concepts de base de S3 (pytest doc, Pydantic doc, Git reflog) |

---

## Auto-√©valuation

**Questions √† se poser apr√®s le quiz** :
- Peux-tu expliquer √† un ami **sans regarder les fichiers** pourquoi pytest est plus rapide que tester manuellement avec curl ?
- Si l'API crashe en production, peux-tu d√©crire ton processus de debugging en 3 √©tapes concr√®tes (logs, reflog, rollback) ?
- Comprends-tu la diff√©rence entre "le code fonctionne" et "le code est test√© automatiquement" ?

Si 3 √ó oui ‚Üí **S3 solidement acquise** üéâ  
Si 2 √ó non ‚Üí Refaire le TP S3 avant S4